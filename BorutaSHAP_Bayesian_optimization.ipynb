{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fda591-21e1-4ec9-8581-5be93dbe6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimization: Selected multi-step classification set\n",
    "##############################\n",
    "######### YM dataset #########\n",
    "##############################\n",
    "\n",
    "# Boruta-SHAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, recall_score, precision_score, f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from BorutaShap import BorutaShap\n",
    "\n",
    "# Creates a BorutaShap selector for regression\n",
    "selector = BorutaShap(model = None, importance_measure = 'shap', classification = True)\n",
    "\n",
    "\n",
    "for k in range(0,2):\n",
    "    print(k)\n",
    "\n",
    "    if k == 0:\n",
    "        ym = pd.read_csv('dt_ym_12_3_4_5_new.csv',';')\n",
    "    elif k == 1:\n",
    "        ym = pd.read_csv('dt_ym_123_4_5_new.csv',';')  \n",
    "        \n",
    "    \n",
    "#   # Accuracy 70% achieved with this multi-steps\n",
    "#     if k == 0:\n",
    "#         ym = pd.read_csv('dt_ym_123_45_new.csv',';')\n",
    "#     elif k == 1:\n",
    "#         ym = pd.read_csv('dt_ym_4_5_new.csv',';')\n",
    "#     elif k == 2:\n",
    "#         ym = pd.read_csv('dt_ym_12_3_new.csv',';')\n",
    "#     elif k == 3:\n",
    "#         ym = pd.read_csv('dt_ym_1_2_new.csv',';')\n",
    "\n",
    "#   # Accuracy 72% achieved with this multi-steps\n",
    "#     if k == 0:\n",
    "#         ym = pd.read_csv('dt_ym_1245_3_new.csv',';')\n",
    "#     elif k == 1:\n",
    "#         ym = pd.read_csv('dt_ym_124_5_new.csv',';')\n",
    "#     elif k == 2:\n",
    "#         ym = pd.read_csv('dt_ym_12_4_new.csv',';')\n",
    "#     elif k == 3:\n",
    "#         ym = pd.read_csv('dt_ym_1_2_new.csv',';')\n",
    "\n",
    "        \n",
    "        \n",
    "    ym.head()\n",
    "\n",
    "    ym_pr = ym\n",
    "    X = ym_pr.drop(columns = ['pref_nominal', 'ac2_nominal', 'ac5_nominal', 'preference', 'action2', 'action5', 'action5_1'])\n",
    "    ym_ac5_1_cl = ym['action5_1']\n",
    "    \n",
    "    y = ym_ac5_1_cl\n",
    "\n",
    "    \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    \n",
    "    \n",
    "############################# basic model training and test result to plot model loss ################################\n",
    "    model_base = XGBClassifier(random_state=42)\n",
    "    evalset = [(X_train, y_train), (X_test, y_test)]\n",
    "    model_base.fit(X_train, y_train, eval_metric='mlogloss', eval_set=evalset)\n",
    "    results_base = model_base.evals_result()\n",
    "#######################################################################################################################    \n",
    "    \n",
    "        \n",
    "\n",
    "    # y_train2 = y_train.map({'prefer': 1, 'not': 0}).astype(int) \n",
    "    # y_pred2 = pd.Series(y_pred2)\n",
    "    # y_pred2 = y_pred2.map({'prefer': 1, 'not': 0}).astype(int) \n",
    "    # y_test = y_test.map({'prefer': 1, 'not': 0}).astype(int)\n",
    "\n",
    "\n",
    "    # Fits the selector\n",
    "    selector.fit(X = X_train, y = y_train, n_trials = 100, sample = False, verbose = True)\n",
    "    # n_trials -> number of iterations for Boruta algorithm\n",
    "    # sample -> samples the data so it goes faster\n",
    "\n",
    "    # Display features to be removed\n",
    "    features_to_remove = selector.features_to_remove\n",
    "    print(features_to_remove)\n",
    "    # Removes them\n",
    "    X_train_boruta_shap = X_train.drop(columns = features_to_remove)\n",
    "    X_test_boruta_shap = X_test.drop(columns = features_to_remove)\n",
    "\n",
    "\n",
    "      \n",
    "    model = XGBClassifier(random_state=42)\n",
    "    evalset = [(X_train_boruta_shap, y_train), (X_test_boruta_shap, y_test)]\n",
    "    # fit the model\n",
    "    model.fit(X_train_boruta_shap, y_train, eval_metric='mlogloss', eval_set=evalset, early_stopping_rounds=30)\n",
    "\n",
    "\n",
    "    #     # y_test = y_test.map({'prefer': 1, 'not': 0}).astype(int)\n",
    "    #     fpr, tpr, _ = roc_curve(y_test.values, pred_p, pos_label=1)\n",
    "    #     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "  \n",
    "    ################################ Bayesian optimization: Hyperparameter ######################################    \n",
    "  \n",
    "    # ym_pr_cl  = ym['pref_nominal']\n",
    "    # ym_pr_cl  = ym['preference']\n",
    "    \n",
    "    y = ym_ac5_1_cl\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train)\n",
    "    y_test = le.fit_transform(y_test)\n",
    "    \n",
    "    \n",
    "    pbounds = {\n",
    "    'learning_rate': (0.01, 0.4),\n",
    "    'n_estimators': (20, 100),\n",
    "    'max_depth': (1,5),\n",
    "    #     'subsample': (1.0, 1.0),  # Change for big datasets\n",
    "    #     'colsample': (1.0, 1.0),  # Change for datasets with lots of features\n",
    "    'gamma': (0, 1)}\n",
    "\n",
    "\n",
    "    def xgboost_hyper_param(learning_rate,\n",
    "                            n_estimators,\n",
    "                            max_depth,\n",
    "    #                         subsample,\n",
    "    #                         colsample,\n",
    "                            gamma):\n",
    "\n",
    "        max_depth = int(max_depth)\n",
    "        n_estimators = int(n_estimators)\n",
    "\n",
    "        clf = XGBClassifier(\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            n_estimators=n_estimators,\n",
    "            gamma=gamma)\n",
    "        return np.mean(cross_val_score(clf, X_train_boruta_shap, y_train, cv=3, scoring='roc_auc_ovr'))\n",
    "\n",
    "    optimizer = BayesianOptimization(\n",
    "        f=xgboost_hyper_param,\n",
    "        pbounds=pbounds,\n",
    "        random_state=1)\n",
    "\n",
    "\n",
    "    # performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
    "    ## optimizer.maximize(n_iter=10, init_points=20, acq='ei')\n",
    "    optimizer.set_gp_params(alpha=1e-3, n_restarts_optimizer=5)\n",
    "    optimizer.maximize(init_points=2, n_iter=10)\n",
    "\n",
    "    #Extracting the best parameters\n",
    "    params = optimizer.max['params']\n",
    "    print(params)\n",
    "\n",
    "    #Converting the max_depth and n_estimator values from float to int\n",
    "    params['max_depth']= int(params['max_depth'])\n",
    "    params['n_estimators']= int(params['n_estimators'])\n",
    "#     learning_rate = 0.05\n",
    "#     params['learning_rate']= int(learning_rate)\n",
    "    \n",
    "\n",
    "    #Initialize an XGBClassifier with the tuned parameters and fit the training data\n",
    "    # from xgboost import XGBClassifier\n",
    "    evalset = [(X_train_boruta_shap, y_train), (X_test_boruta_shap, y_test)]\n",
    "    model2 = XGBClassifier(**params).fit(X_train_boruta_shap, y_train, eval_metric='mlogloss', eval_set=evalset, early_stopping_rounds=30)\n",
    "\n",
    "    #predicting for training set\n",
    "    y_pred2 = model2.predict(X_train_boruta_shap)\n",
    "\n",
    "    #Looking at the classification report\n",
    "    print(classification_report(y_pred2, y_train))\n",
    "    \n",
    "        \n",
    "    # pandas.Series\n",
    "    score = model2.score(X_test_boruta_shap, y_test)\n",
    "\n",
    "    # make predictions for test data\n",
    "    y_pred2 = model2.predict(X_test_boruta_shap)\n",
    "    pred_p2 = model2.predict_proba(X_test_boruta_shap)\n",
    "\n",
    "    y_pred2 = pd.Series(y_pred2)\n",
    "#     y_pred2 = y_pred2.map({'prefer': 1, 'not': 0}).astype(int) \n",
    "#     y_test = y_test.map({'prefer': 1, 'not': 0}).astype(int)\n",
    "    valid_score = roc_auc_score(y_test, pred_p2, multi_class=\"ovr\")\n",
    "\n",
    "    #Looking at the classification report\n",
    "    print(classification_report(y_pred2, y_test))\n",
    "    \n",
    "    \n",
    "#     print('Precision: %.3f' % precision_score(y_test, y_pred2))\n",
    "#     print('Recall: %.3f' % recall_score(y_test, y_pred2))\n",
    "#     print('F1 Score: %.3f' % f1_score(y_test, y_pred2))\n",
    "#     print('Accuracy: %.3f' % accuracy_score(y_test, y_pred2))\n",
    "#     print('Validation ROC-AUC score: %.3f' % valid_score)\n",
    "     \n",
    "        \n",
    "    # retrieve performance metrics\n",
    "    results = model2.evals_result()\n",
    "\n",
    "    if k == 0:    \n",
    "        young_results = model2.evals_result()\n",
    "        young_params = params\n",
    "\n",
    "    elif k == 1:\n",
    "        old_results = model2.evals_result()\n",
    "        old_params = params\n",
    "    \n",
    "    elif k ==2:\n",
    "        comb_results = model2.evals_result()\n",
    "        comb_params = params    \n",
    "    \n",
    "    \n",
    "    # plot learning curves\n",
    "    pyplot.plot(results['validation_0']['mlogloss'], label='train')\n",
    "    pyplot.plot(results['validation_1']['mlogloss'], label='test')\n",
    "    # show the legend\n",
    "    pyplot.legend(fontsize=14)\n",
    "    pyplot.xlabel('Epoch', fontsize=16)\n",
    "    pyplot.ylabel('Model loss', fontsize=16)\n",
    "    pyplot.xlim([-5, 170])\n",
    "    pyplot.ylim([0, 0.7])\n",
    "    pyplot.xticks(fontsize = 14)\n",
    "    pyplot.yticks(fontsize = 14)\n",
    "#     pyplot.title(age_group + ': ' + 'model loss', fontsize=16)\n",
    "    \n",
    "    pyplot.show()\n",
    "    \n",
    "    \n",
    "    selector.plot(X_rotation=90,which_features='all', X_size = 13, figsize=(5,8))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
